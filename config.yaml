# Fano Explorer Configuration

# Browser settings
browser:
  # Path to Chrome executable (leave empty for auto-detect)
  chrome_path: ""
  # Whether to run headless (invisible) - set to false for debugging
  headless: false
  # Directory to store browser sessions
  user_data_dir: "./browser_data"
  # Slow down operations for stability (milliseconds)
  slow_mo: 100
  # Viewport size (smaller to avoid window cutoff on Windows)
  viewport_width: 1280
  viewport_height: 720

# Model endpoints
models:
  chatgpt:
    url: "https://chat.openai.com"
    # Selectors for detecting states (CSS selectors)
    selectors:
      input: "textarea[data-id='root']"
      send: "button[data-testid='send-button']"
      response: "div[data-message-author-role='assistant']"
      rate_limit_patterns:
        - "You've reached the limit"
        - "Usage cap reached"
        - "try again later"
    # How long to wait for response (seconds) - Pro mode can be slow
    response_timeout: 3600
    
  gemini:
    url: "https://gemini.google.com"
    selectors:
      input: "rich-textarea"
      send: "button.send-button"
      response: "message-content"
      deep_think_toggle: "button[aria-label='Deep Think']"
      rate_limit_patterns:
        - "unavailable"
        - "try again tomorrow"
        - "quota"
    response_timeout: 3600  # Deep Think can be slow

# Model selection weights (higher = more likely to be chosen)
# Gemini tends to explore creatively, ChatGPT tends to critique rigorously
model_weights:
  exploration:
    gemini: 60
    chatgpt: 40
  critique:
    gemini: 30
    chatgpt: 70
  synthesis:
    gemini: 50
    chatgpt: 50

# Deep/Pro mode daily limits (preserved for synthesis and breakthroughs)
deep_modes:
  gemini_deep_think:
    daily_limit: 20
  chatgpt_pro:
    daily_limit: 100

# Orchestration settings
orchestration:
  # Seconds between exploration cycles
  poll_interval: 30
  # Seconds to wait when all models rate-limited
  backoff_base: 300  # 5 minutes
  backoff_max: 3600  # 1 hour max
  # How many exchange rounds before considering chunk-ready
  min_exchanges_for_chunk: 4
  # Maximum exchanges before forcing synthesis
  max_exchanges_per_thread: 20
  # Number of concurrent exploration threads
  max_active_threads: 3

# Chunk synthesis settings
synthesis:
  # Minimum critique rounds before chunk is ready
  min_critiques: 2
  # Keywords that suggest profundity (boost chunk-readiness)
  profundity_signals:
    - "this explains"
    - "naturally follows"
    - "must be"
    - "inevitable"
    - "elegant"
    - "beautiful"
    - "symmetric"
  # Keywords that suggest problems (delay chunk-readiness)  
  doubt_signals:
    - "but wait"
    - "doesn't quite"
    - "forcing"
    - "arbitrary"
    - "why would"
    - "seems ad hoc"

# Review server settings
review_server:
  host: "127.0.0.1"
  port: 8765

# Atomic chunking settings
chunking:
  # "atomic" extracts multiple insights per thread, "monolithic" is old behavior
  mode: "atomic"
  # Filter out insights below this confidence: "high", "medium", or "low"
  min_confidence_to_keep: "low"
  # Maximum insights to extract per thread
  max_insights_per_thread: 10

# Dependency resolution settings
dependencies:
  # Keyword overlap threshold for matching deps to blessed chunks
  semantic_match_threshold: 0.5
  # Maximum blessed insights to include in extraction prompt
  max_blessed_in_prompt: 20
  # Flag during review if insight has unresolved dependencies
  warn_on_unresolved: true

# Deduplication settings (prevents near-duplicate insights)
deduplication:
  # Enable deduplication checking
  enabled: true
  # Keyword Jaccard similarity threshold for flagging potential duplicates
  keyword_threshold: 0.6
  # Concept overlap threshold (domain-specific terms)
  concept_threshold: 0.7
  # Use LLM to confirm duplicates (slower but more accurate)
  use_llm_confirmation: true

# Automated review panel settings
review_panel:
  # Enable automated LLM review panel
  enabled: true
  # Environment variable containing Claude API key
  claude_api_key_env: "ANTHROPIC_API_KEY"
  # Claude model to use (Opus for extraction/refinement, best at precise articulation)
  claude_model: "claude-opus-4-20250514"

  # Round 1: Independent review (standard modes)
  round1:
    use_deep_modes: false

  # Refinement settings (Claude Opus rewrites based on critiques)
  refinement:
    # Maximum refinement attempts before escalating to deliberation
    max_refinement_rounds: 2
    # If true, escalate to deliberation after failed refinement
    escalate_after_failed_refinement: true

  # Round 2: Deep analysis (deep think modes)
  round2:
    use_deep_modes: true

  # Round 3: Structured deliberation
  round3:
    enabled: true
    max_exchanges: 3

  # Outcome handling
  outcomes:
    unanimous_bless: "auto_bless"
    unanimous_reject: "auto_reject"
    unanimous_uncertain: "needs_development"
    disputed_majority_bless: "bless_with_flag"
    disputed_majority_reject: "reject_with_flag"

# Chunk augmentation settings
augmentation:
  # Enable augmentation of blessed insights
  enabled: true
  # Automatically generate augmentations for all blessed chunks
  auto_generate: true
  # Must pass verification before attaching
  require_verification: true

  # Type-specific settings
  types:
    diagram:
      enabled: true
      format: "svg"  # or "png"
      tool: "matplotlib"  # or "graphviz"
    table:
      enabled: true
      format: "markdown"
    proof:
      enabled: true
      require_review: true  # Second LLM must verify
    code:
      enabled: true
      execute: true  # Actually run the code
      timeout_seconds: 30

# Logging
logging:
  level: "INFO"  # DEBUG, INFO, WARNING, ERROR
  file: "./logs/exploration.log"
  max_bytes: 10485760  # 10MB
  backup_count: 5
